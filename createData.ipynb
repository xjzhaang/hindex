{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d1824",
   "metadata": {},
   "source": [
    "# Importing data and extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e06d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "df_test = pd.read_csv('test.csv', dtype={'author': np.int64})\n",
    "n_test = df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graphs(graph):\n",
    "    G = nx.read_edgelist(graph, delimiter=' ', nodetype=int)\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_edges = G.number_of_edges()\n",
    "    return G, n_nodes, n_edges\n",
    "def read_adj_graphs(graph):\n",
    "    G = nx.read_multiline_adjlist(graph, nodetype=int)\n",
    "    return G\n",
    "def read_weighted_graphs(graph):\n",
    "    G = nx.read_edgelist(graph, nodetype=int, data=((\"weight\", float),))\n",
    "    return G\n",
    "\n",
    "def compute_sim_features(G):\n",
    "    pagerank = nx.pagerank(G)\n",
    "    avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
    "    triangles = nx.triangles(G)\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "    return pagerank, eigenvector_centrality,triangles,avg_neighbor_degree\n",
    "\n",
    "def compute_graph_features(G):\n",
    "    core_number = nx.core_number(G) \n",
    "    clustering = nx.clustering(G)\n",
    "    pagerank = nx.pagerank(G)\n",
    "    avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
    "    onion_layers = nx.onion_layers(G)\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    return core_number, degree_centrality, clustering, avg_neighbor_degree, onion_layers, pagerank  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_coauthor, co_nodes, co_edges = read_graphs(\"coauthorship.edgelist\")\n",
    "G_sum_sim = read_adj_graphs(\"sum_sim_authors.adjlist\")\n",
    "G_weighted = read_weighted_graphs(\"weighted_coauthorship.edgelist\")\n",
    "co_a, co_b, co_c, co_d, co_e, co_f = compute_graph_features(G_coauthor)\n",
    "w_a, w_b, w_c, w_d = compute_sim_features(G_weighted)\n",
    "ms_a, ms_b, ms_c, ms_d = compute_sim_features(G_sum_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062dfb43",
   "metadata": {},
   "source": [
    "## Extract coauthor h_index data from graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711065b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract max, min and mean coauthor hindex\"\"\"\n",
    "df_dict = df_train.to_dict() \n",
    "inv_df = {v: k for k, v in df_dict[\"author\"].items()}\n",
    "def compute_mean_max_coauthor_hindex(graph, node):\n",
    "    df_coauth_hindex = [df_dict[\"hindex\"].get(key1) for key1 in [inv_df.get(key) for key in [x for x in graph.neighbors(node)]] if df_dict[\"hindex\"].get(key1) is not None]\n",
    "    if df_coauth_hindex == []:\n",
    "        max_hind = 0\n",
    "        min_hind = 0\n",
    "        mean_hind = 0\n",
    "    else:\n",
    "        max_hind = np.max(df_coauth_hindex)\n",
    "        min_hind = np.min(df_coauth_hindex)\n",
    "        mean_hind = np.mean(df_coauth_hindex)\n",
    "    return max_hind, min_hind, mean_hind, df_coauth_hindex\n",
    "\n",
    "\"\"\"Extract number of paper the author and on average how many its coauthor has\"\"\"\n",
    "file = open('author_papers.txt', encoding = 'utf8')\n",
    "Author_paper_num = {}\n",
    "Author_paper = {}\n",
    "for i in tqdm(range(co_nodes)):\n",
    "    newline = file.readline()\n",
    "    author = newline.split(':')[0]\n",
    "    top5 = newline.split(':')[1].strip().split(\"-\")\n",
    "    Author_paper[int(author)] = top5\n",
    "    Author_paper_num[int(author)] = len(top5)\n",
    "    \n",
    "    \n",
    "def mean_coauthor_paper_count(nodes):\n",
    "    mean_co = {}\n",
    "    for node in nodes:\n",
    "        if len(list(G_coauthor.neighbors(node))) == 0:\n",
    "            mean_co[node] = 0        \n",
    "        p = 0\n",
    "        for n in list(G_coauthor.neighbors(node)):\n",
    "            p += len(Author_paper[n])\n",
    "        mean_co[node] = p / len(list(G_coauthor.neighbors(node)))\n",
    "    return mean_co\n",
    "\n",
    "nodes = list(G_coauthor.nodes())\n",
    "mean_co = mean_coauthor_paper_count(nodes)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5a260",
   "metadata": {},
   "source": [
    "## Import Deepwalk embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_auth_emb = {}\n",
    "filename = 'DeepWalkEmbeddings/coauthor.embeddings'\n",
    "file = open(filename, encoding = 'utf8')\n",
    "N_lines, n_dim2 = file.readline().strip().split(\" \")\n",
    "N_lines = int(N_lines); n_dim2 = int(n_dim2)\n",
    "for i in range(N_lines):\n",
    "    a = file.readline().split(' ')\n",
    "    auth = a[0]\n",
    "    rest = a[1:]\n",
    "    co_auth_emb[auth] = np.array(rest).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ae2143",
   "metadata": {},
   "source": [
    "## Import Doc2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d21d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dv = Doc2Vec.load(\"d2v_50_new/d2v_model_PV-DM\")\n",
    "model_dbow = Doc2Vec.load(\"d2v_50_new/d2v_model_PV-DBOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218280a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linecount = open('abstracts.txt')\n",
    "file = open('abstracts.txt', encoding='utf8')\n",
    "ids = []\n",
    "for i in tqdm(range(624181)):\n",
    "    newLine = file.readline()\n",
    "    split = newLine.split('----', 1)\n",
    "    ids.append(split[0])\n",
    "    \n",
    "embedding_id_to_ids = { i : ids[i] for i in range(len(ids)) }\n",
    "ids_to_embedding_id = dict((v, k) for k, v in embedding_id_to_ids.items())\n",
    "mean_dict_dv = {}\n",
    "mean_dict_dbow = {}\n",
    "sum_dict_dv = {}\n",
    "sum_dict_dbow = {}\n",
    "for key in tqdm(Author_paper.keys()):\n",
    "    list_id = [i for i in Author_paper[key]]\n",
    "    \n",
    "    emb = []\n",
    "    emb2 = []\n",
    "    for i in list_id:\n",
    "        try:\n",
    "            code = ids_to_embedding_id[i]\n",
    "            emb.append(model_dv.dv[code])\n",
    "            emb2.append(model_dbow.dv[code])\n",
    "        except:\n",
    "            pass\n",
    "    mean_dv = np.mean(emb,axis=0)\n",
    "    sum_dv = np.sum(emb,axis=0)\n",
    "    mean_dict_dv[key] = mean_dv\n",
    "    sum_dict_dv[key] = sum_dv\n",
    "    \n",
    "    mean_db = np.mean(emb2,axis=0)\n",
    "    sum_db = np.sum(emb2,axis=0)\n",
    "    mean_dict_dbow[key] = mean_db\n",
    "    sum_dict_dbow[key] = sum_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562cdd8",
   "metadata": {},
   "source": [
    "## Import Node2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_weighted_n2v = KeyedVectors.load_word2vec_format('Node2VecEmb/n2v_g_weighted.nodevectors')\n",
    "g_sim_n2v = KeyedVectors.load_word2vec_format(\"Node2VecEmb/n2v_g_sim.nodevectors\")\n",
    "g_sim_n2v_ = {}\n",
    "for g in tqdm(G_coauthor.nodes()):\n",
    "    try: \n",
    "        g_sim_n2v_[g] = g_sim_n2v[str(g)]\n",
    "    except:\n",
    "        g_sim_n2v_[g] = np.zeros(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2adf1a",
   "metadata": {},
   "source": [
    "# Building Train, Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0852143",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "dim_2 = 100\n",
    "X_train = np.zeros((n_train,  3 * dim + 22))\n",
    "y_train = np.zeros(n_train)\n",
    "for i, row in tqdm(df_train.iterrows()):\n",
    "    node = row['author']\n",
    "    max_hind, min_hind, mean_hind, df_coauth_hindex = compute_mean_max_coauthor_hindex(G_coauthor, node)\n",
    "    X_train[i, 0] = co_a[node]\n",
    "    X_train[i, 1] = co_b[node]\n",
    "    X_train[i, 2] = co_c[node]\n",
    "    X_train[i, 3] = co_d[node] \n",
    "    X_train[i, 4] = co_e[node] \n",
    "    X_train[i, 5] = co_f[node] \n",
    "    X_train[i, 6] = ms_a[node]\n",
    "    X_train[i, 7] = ms_b[node]\n",
    "    X_train[i, 8] = ms_c[node]\n",
    "    X_train[i, 9] = ms_d[node]\n",
    "    X_train[i, 10] = w_a[node]\n",
    "    X_train[i, 11] = w_b[node]\n",
    "    X_train[i, 12] = w_c[node]\n",
    "    X_train[i, 13] = w_d[node]\n",
    "    X_train[i, 14] = (max_hind - min_hind)\n",
    "    X_train[i, 15] = np.sum(df_coauth_hindex)\n",
    "    X_train[i, 16] = mean_hind\n",
    "    X_train[i, 17] = Author_paper_num[node]\n",
    "    X_train[i, 18] = mean_co[node]\n",
    "    X_train[i, 19] = G_coauthor.degree(node)\n",
    "    X_train[i, 20] = G_sum_sim.degree(node)\n",
    "    X_train[i, 21] = G_weighted.degree(node)\n",
    "    X_train[i, 22:dim + 22] = sum_dict_dbow[node]\n",
    "    X_train[i, dim + 22 :  2 * dim + 22] = sum_dict_dv[node]\n",
    "    #X_train[i, dim + 22 : 2 * dim + 22] = sum_dict_dv[node]\n",
    "    #X_train[i, 2 * dim + 22:  2 * dim + 22 +64] = co_auth_emb[auth]\n",
    "    X_train[i, 2 * dim + 22 : 2 * dim + 22 + dim] = g_weighted_n2v[str(int(node))]\n",
    "    #X_train[i, 2 * dim + 22 + dim :3 * dim + 22 + dim] = g_sim_n2v_[int(node)]\n",
    "    #X_train[i, 2 * dim + 22: 2 * dim + 22 + dim_2] = g_weighted_n2v[str(int(node))]\n",
    "    #X_train[i, 2 * dim + 22 : 3 * dim + 22] = g_sim_n2v_[int(node)]\n",
    "    #X_train[i, 2 * dim + 22 + dim_2 : 2 * dim + 22 + 2 * dim_2] = g_weighted_n2v[str(int(node))]\n",
    "    #X_train[i, 2 * dim_2 + dim + 21: 2 * dim_2 + 2 * dim + 21] = model_dbow_sum[node]\n",
    "    y_train[i] = row[\"hindex\"]\n",
    "    \n",
    "X_test = np.zeros((n_test,  3 * dim + 22))\n",
    "for i, row in tqdm(df_test.iterrows()):\n",
    "    node = row['author']\n",
    "    max_hind, min_hind, mean_hind, df_coauth_hindex = compute_mean_max_coauthor_hindex(G_coauthor, node)\n",
    "#     X_test[i, dim] = sum_dict_dv[node]\n",
    "#     X_test[i, dim : 2 * dim] = sum_dict_dbow[node]\n",
    "    X_test[i, 0] = co_a[node]\n",
    "    X_test[i, 1] = co_b[node]\n",
    "    X_test[i, 2] = co_c[node]\n",
    "    X_test[i, 3] = co_d[node] \n",
    "    X_test[i, 4] = co_e[node] \n",
    "    X_test[i, 5] = co_f[node] \n",
    "    X_test[i, 6] = ms_a[node]\n",
    "    X_test[i, 7] = ms_b[node]\n",
    "    X_test[i, 8] = ms_c[node]\n",
    "    X_test[i, 9] = ms_d[node]\n",
    "    X_test[i, 10] = w_a[node]\n",
    "    X_test[i, 11] = w_b[node]\n",
    "    X_test[i, 12] = w_c[node]\n",
    "    X_test[i, 13] = w_d[node]\n",
    "    X_test[i, 14] = (max_hind - min_hind)\n",
    "    X_test[i, 15] = np.sum(df_coauth_hindex)\n",
    "    X_test[i, 16] = mean_hind\n",
    "    X_test[i, 17] = Author_paper_num[node]\n",
    "    X_test[i, 18] = mean_co[node]\n",
    "    X_test[i, 19] = G_coauthor.degree(node)\n",
    "    X_test[i, 20] = G_sum_sim.degree(node)\n",
    "    X_test[i, 21] = G_weighted.degree(node)\n",
    "    X_test[i, 22:dim + 22] = sum_dict_dbow[node]\n",
    "    X_test[i, dim + 22 :  2 * dim + 22] = sum_dict_dv[node]\n",
    "    #X_test[i, 2 * dim + 22 : 3 * dim + 22] = g_sim_n2v_[int(node)]\n",
    "    #X_test[i, 2 * dim + 22:  2 * dim + 22 +64] = co_auth_emb[auth]\n",
    "    X_test[i, 2 * dim + 22 : 2 * dim + 22 + dim] = g_weighted_n2v[str(int(node))]\n",
    "    #X_test[i, dim_2 + 22: dim_2 + 22 + 64] = co_auth_emb[auth]\n",
    "    #X_test[i, 2 * dim + 22 + dim :3 * dim + 22 + dim] = g_sim_n2v_[int(node)]\n",
    "    #X_test[i, dim_2 + 21: 2 * dim_2 + 21] = g_sim_n2v_[int(node)]\n",
    "    #X_test[i, 2 * dim_2 + dim + 21: 2 * dim_2 + 2 * dim + 21] = model_dbow_sum[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).to_csv('X_train.csv',index=False)\n",
    "pd.DataFrame(X_test).to_csv('X_test.csv',index=False)\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feddb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nan = np.nan_to_num(X_train)\n",
    "X_test_nan = np.nan_to_num(X_test)\n",
    "X_train_1 = scaler.fit_transform(X_train_nan)\n",
    "X_test_1 = scaler.fit_transform(X_test_nan)\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X_train_1, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GridSearchCV pipeline found online \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
    "                       do_probabilities = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring_fit,\n",
    "        verbose=2\n",
    "    )\n",
    "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "    \n",
    "    if do_probabilities:\n",
    "          pred = fitted_model.predict_proba(X_test_data)\n",
    "    else:\n",
    "          pred = fitted_model.predict(X_test_data)\n",
    "    \n",
    "    return fitted_model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1715f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMRegressor\n",
    "model = LGBMRegressor(n_jobs=12)\n",
    "param_grid = {\n",
    "    'n_estimators': [7000, 8000, 9000, 10000],\n",
    "    'max_depth': [10, 12, 14, 20],\n",
    "    'num_leaves': [16, 24, 32],\n",
    "    'learning_rate': [0.006, 0.008, 0.01, 0.012, 0.015, 0.02,0.03],\n",
    "    'reg_lambda': [0.1, 0.3, 0.7]\n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_cv = RandomizedSearchCV(\n",
    "    model, param_grid, n_iter=5, cv=3, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "_ = random_cv.fit(X_train_,y_train_)\n",
    "# model, pred = algorithm_pipeline(X_train_, X_test_, y_train_, y_test_, model, \n",
    "#                                  param_grid, cv=3, scoring_fit='accuracy')\n",
    "\n",
    "print(random_cv.best_score_)\n",
    "print(random_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c548bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
